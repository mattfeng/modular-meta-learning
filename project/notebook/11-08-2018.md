# Week of Nov. 8 to Nov. 14

## Recap
* Read [Policy Gradient Methods for Reinforcement Learning with Function Approximation (Sutton, 2000).](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)
<hr/>

## Goals
#### Code
* Implement GridWorld environment with OpenAI Gym (11/08/2018)
* Have a working implementation of VIN on the GridWorld environment (11/09/2018 &ndash; have an implementation training)

#### Theory
* Read [William's original REINFORCE paper (Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning).](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)
* Read Sutton's chapter on Policy Gradient methods
* Read and summarize [Kaelbling's RL Survey.](https://www.cs.cmu.edu/~tom/10701_sp11/slides/Kaelbling.pdf)
* Read and summarize [Proximal Policy Optimization paper.](https://arxiv.org/pdf/1707.06347.pdf)
* Read and summarize Natural Gradients papers.
    * [Revisiting natural gradient for deep networks](https://arxiv.org/pdf/1301.3584.pdf)
    * [New insights and perspectives on the natural gradient method](https://arxiv.org/pdf/1412.1193.pdf)
* Read and summarize [TRPO paper.](https://arxiv.org/pdf/1502.05477.pdf)

## Questions
* Should the model use a value-based approach or a policy gradient method? 
* Where exactly does the 

<hr/>

## 11/08
### Summary.
### Details.

<hr/>

## 11/09
### Summary.
### Details.

<hr/>

## 11/10
### Summary.
### Details.

<hr/>

## 11/11
### Summary.
### Details.

<hr/>

## 11/12
### Summary.
### Details.

<hr/>

## 11/13
### Summary.
### Details.

<hr/>

## 11/14
### Summary.
### Details.

